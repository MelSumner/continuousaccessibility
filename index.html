<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0,user-scalable=yes">
  <title>Continuous Accessibility</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=Inter:wght@300;600&family=Playfair+Display:ital,wght@0,500;1,500&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="assets/style.css">

</head>

<body>
  <header class="header">
    <h1 class="fluid-type">Continuous Accessibility</h1>
  </header>
  <main class="main">
    <article>

      <p class="intro">Continuous Accessibility is defined as the approach to ensuring that code intended to be
        displayed in browsers can be continuously checked and monitored for digital accessibility requirements through a
        novel application of existing software engineering concepts and Web Content Accessibility Guidelines (WCAG).</p>
      <h2 class="fluid-type">Historical challenges</h2>
      <p>There is not a sufficient way to quantify the full scope of digital accessibility (a11y) work, and an external
        best practice for accessibility quantification to imitate does not currently exist.</p>
      <p>Additionally, these common challenges frequently exist:</p>
      <ul>
        <li>Limited visibility into the audit process and results. A11y audit results are typically only available for a
          few people. The results/reports themselves are often complex and difficult to use.</li>
        <li>Incomplete audit process. Often there is no context provided on an a11y scorecard/dashboard, so folks don’t
          have the correct insights needed to improve.</li>
        <li>We lack a sufficient way to prevent regressions for tested code; while technical solutions for regression
          and snapshot testing exist, they are largely ineffective because they are not trained to recognize regression
          significance and often do not have policy support.</li>
        <li>Static (linting) and dynamic (testing) checks can be turned off at the application level (i.e., linting
          rules turned off in a product’s template lint config file), at the line level (a developer can turn off a
          linting rule in their code) and in other places</li>
        <li>Lack of incentive for developers to create accessible code. We have yet to align development practices such
          that accessibility is taken care of as part of doing the normal feature work.</li>
        <li>Lack of metrics. As subject matter experts, we have the lived experience to know what kind of work needs to
          be done. Clearly-defined metrics will give management and product teams the math they need to both understand
          and support our work.</li>
        <li>The work is complex. In this case, complexity is calculated by the number of interactions that must take
          place for the work to be completed, the amount of time it will take to complete the project, and that some of
          the methods still need to be invented since they do not currently exist.</li>
        <li>Inability to benchmark. Accessibility is in a different kind of zone from other software engineering
          specialties, due to the legal requirements for digital accessibility. Avoiding the risk of legal liability is
          just one of the reasons that no industry-wide benchmarks exist.</li>
      </ul>
      <hr>
      <blockquote class="fluid-type">Digital accessibility can be hard.
        Let's automate it.</blockquote>
      <hr>
      <h2 class="fluid-type">The approach</h2>
      <p>So, how do we do this, exactly? </p>
      <p>Comprehensive accessibility conformance for digital products represents an unsolved, complex problem and known
        to be without a reliable, transparent route to success. Further adding to the complexity, standards are still
        being developed for native applications on devices such as mobile phones and wearables.</p>
      <p>In practice, achieving a given WCAG criterion requires that a set of actions, implementations, or policy
        updates that individually ensure for some portion of the success criterion (e.g., a particular use case) is met.
        As such, the most effective means of generating rapid, high-impact a11y improvements for digital products is to
        deconstruct a WCAG success criterion into a subset of a11y QC mechanisms: action steps that can be taken to
        comprehensively achieve an accessible experience.</p>
      <p>For example, automated linting tools are currently employed to detect the presence of nested interactive
        elements, which generate a focus-related failure of WCAG Success Criterion 1.3.1: Info and Relationships.</p>
      <p>The process by which efficient improvements can be achieved matches that of the software development process in
        general -- tightly scoped, iterative, and distributed.</p>
      <p>So what could these metrics look like?</p>
      <h3>Metric: Total Criteria Count (TCC)
      <span class="subtitle">Actionable Outcome(s): Goal Setting</span>
    </h3>
      <p>
        The Total Criteria Count (TCC) is the first item to establish as it will be the basis for all our other metrics.
        It is also likely to take the most amount of time to establish (relative to other metrics). We must establish an
        exact count of all WCAG criteria with which our products must comply- with the full understanding that this
        exact count may change over time (and changes should be tracked). </p>
      <p>
        This should be an itemized list that includes:</p>
      <ul>
        <li>WCAG criteria</li>
        <li>Criteria that are not already reflected in WCAG criteria (perhaps related to tooling, not covered by web
          content guidelines)</li>
        <li>Known techniques</li>
        <li>Common failures</li>
        <li>Failures as identified by audit findings</li>
      </ul>
      <p>As our ability to identify root causes increases, we should be able to add to this list. The more detail we have,
      the better it will be, since success criteria cover generalities rather than specifics. For example, WCAG 1.3.1
      (Info and Relationships) is a single success criterion but relates to 25+ different failure scenarios. It may not
      be meaningful enough to simply indicate a decrease in issues related to a single WCAG success criterion; the
      metrics may need to be more granular than that. </p>
      <p>
      The list MUST be easily available and have the ability to track change over time.</p>
      <hr>
      <h3>
        Metric: Automation Capabilities
        <span class="subtitle">Actionable Outcome(s): Problem Analysis, Goal Setting</span>
      </h3>
      <p>We must then identify additional baseline numbers from TCC line items, related to automation:
        Criteria for which we can provide automated linting (static analysis) (Automated Linting Criteria Count (ALCC))
        Criteria for which we can provide automated testing (dynamic analysis) (Automated Testing Criteria Count (ATCC))
        Criteria that require developer-authored tests (Developer Test Criteria Count (DTCC))
        Criteria that require manual testing (Manual Test Criteria Count (MTCC))
        We should determine which line items from TCC currently require manual testing but could reasonably have automated tests. This can help to determine future work. This number may also change over time, given a few variables: 
        Reduction in possible # of ways to approach any given UI component (either self-imposed or via engineering policy)
        Deeper analysis of a line item may show that the status may change (from “possible to automate” to “manual check” or vice versa)
        Changes to the success criteria (as WCAG grows to include devices and not just web, etc.).
        </p>
      <hr>
      <h3>Metric: Audit Results
        <span class="subtitle">Actionable Outcome(s): Problem Analysis, Trend Development</span>
      </h3>
      <p>If apps are audited and scorecards are produced, these metrics are indicators of overall progress. As such, we must analyze them from a few angles:</p>
      <ul>
        <li>What is the Total Bug Count? (TBC)? </li>
          <li>Further breakdowns may be justifiable:
          <ul>
            <li>By Line of Business (LOB)</li>
              <li>By WCAG Success Criteria (WSC)</li>
            </ul>
                <li>Valid Bug Count (VBC) (it may be more meaningful to look at VBC than TBC, or at least normalize the data to account for the mean)</li>
                <li> Bug Severity Count (ISC)
        <ul>
          <li>For bugs that are violations of WCAG conformance:
        Blocker
        Critical
        Major
        Minor</li>
        </ul></li>
        <li>
        How long does it take each LOB to resolve violations? What is the amount of Time To Resolution? (TTR)</li>
        <li>
        What percent of the resolved violations are within the SLA for resolution?</li>
        <li> 
        What are the blockers to violation resolution (is there a recognizable pattern that could inform tooling)?</li>
        <li>
        What is the severity of those blockers?</li>
        <li> 
        Is there a WCAG violation that seems to appear more than the others? Relative Incident Frequency (RIF)</li>
        <li> 
        What LOBs have exemptions? How many? Why? This can inform process improvements. Conformance Exemption Count (CEC)</li>
        <li>Linting Automation Suppression Count (LASC)</li>
        <li>Testing Automation Suppression Count (TASC)</li>
        </p>
        <p>It is necessary to understand the audit process. This will show how the proposed metrics can be useful both now and in the long-term. Typically, manual testing is done on specific workflows for each product, and those numbers are then fed into an audit report. This means that it would be useless at first to have a process to deal with an uptick in reported issues, since it only indicates that the audit has occurred. 
          However, we should be able to implement improved monitoring as we increase our automated testing capabilities. Improved monitoring will allow us to notify teams sooner if a product’s digital a11y conformance deteriorates at a too-rapid rate (Conformance Deterioration Rate (CDR)). 
          Additionally, by obtaining the baseline metrics for audits, we should be able to identify training opportunities for each team. Teams with high audit numbers should automatically qualify for a11y training (A11y Training Threshold (ATT)). 
          These audit-related metrics could also inform processes from a business/legal perspective, as they can be used to quantify risk (of legal action). When the risk is quantified, we can then quantify the reduction in that risk by taking specific steps to remedy the issues; while this may be a business justification, the result is that our users have an improved member experience.
          Finally, and perhaps most importantly, we can use these numbers to see what the persistent issues are. By this, we mean, which violations happen the most often but for which we have no way to test (or prevent) a11y issues? In these instances, we could consider a body of work where the component (simple or complex) is built around the ability to test specific accessibility criteria. A horizontal initiative (or other appropriate action) could then be used so teams can refactor their code to use the accessible replacement component. 
          </p>
      <hr>
      <h3>Metric: Internal Developer A11y Training
        <span class="subtitle">Actionable Outcome(s): Process Improvement</span>
      </h3>
      <p>Track: 
        Total UI Developer Count (TDC) – [TBD, frequency of update? Data must be accurate to be useful- what range is considered acceptable?]
        If a developer has received a11y-specific training while at the company [Question: we have training(s) they can attend, but do we have a certification they can also complete?]
        Product team that the developer is assigned to 
        If the developer has gone through something like an a11y champions program
        What we want to learn
        Hypothesis: required a11y-focused training for FE developers would reduce the number of accessibility issues in our codebase. 
        When used in conjunction with tracking the TBC (or VBC) per line of business (LOB) or product team, we can determine if there is a reduction in reported accessibility issues for that product for teams who have received training.
        This data could be made available to promotion committees to be used in craft quality measurements. For developers who have received training, is there an improvement in code quality? If so, we will be able to make the case for scheduled, mandatory accessibility training for engineers. Furthermore, if the data suggests that teams with alumni of an A11y Champions Program produce products with greater quality, there can be a business value that can be assigned to the investment in the program.
        </p>
      <hr>
      <h3>Metric: Internal Developer Support
        <span class="subtitle">Actionable Outcome(s): Problem Analysis, Trend Development</span>
      </h3>
      <p>One of the ways we could identify common problem areas for developers is by developing the means to analyze the a11y-related channels in Slack. With sentiment analysis (or similar), we could determine what the commonly asked questions are, for two reasons: 
        Some sort of automated tool/bot that could provide answers to common questions or provide links to relevant reference materials would reduce the amount of manual support required. 
        The commonly asked questions could also be indicative of a problem area and could inform future tooling work. If we see the same question asked 100 times, it’s an indication that a broadly applied solution could be needed. 
        We could then track the chat trends over time, to see if there are indicators of trends in the correct direction based on our actions. 
        </p>
        <h2 class="fluid-type">Trend Analysis</h2>
    </article>
  </main>
  <footer class="footer">
    <p>
      <small>
        &copy; 2019 Melanie Sumner.
      </small>
    </p>
  </footer>
</body>

</html>